<html><head></head><body><div class="apidocDiv">
<style>
/*csslint
*/
.apidocDiv {
    background: #fff;
    font-family: Arial, Helvetica, sans-serif;
}
.apidocDiv a[href] {
    color: #33f;
    font-weight: bold;
    text-decoration: none;
}
.apidocDiv a[href]:hover {
    text-decoration: underline;
}
.apidocCodeCommentSpan {
    background: #bbf;
    color: #000;
    display: block;
}
.apidocCodeKeywordSpan {
    color: #d00;
    font-weight: bold;
}
.apidocCodePre {
    background: #eef;
    border: 1px solid;
    color: #777;
    padding: 5px;
    white-space: pre-wrap;
}
.apidocFooterDiv {
    margin-top: 20px;
    text-align: center;
}
.apidocModuleLi {
    margin-top: 10px;
}
.apidocSectionDiv {
    border-top: 1px solid;
    margin-top: 20px;
}
.apidocSignatureSpan {
    color: #777;
    font-weight: bold;
}
</style>
<h1>api documentation for
    <a href="https://github.com/cgiffard/node-simplecrawler">simplecrawler (v1.1.1)</a>
</h1>
<h4>Very straightforward, event driven web crawler. Features a flexible queue interface and a basic cache mechanism with extensible backend.</h4>
<div class="apidocSectionDiv"><a href="#apidoc.tableOfContents" id="apidoc.tableOfContents"><h1>table of contents</h1></a><ol>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler">module simplecrawler</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.simplecrawler">
            function <span class="apidocSignatureSpan"></span>simplecrawler
            <span class="apidocSignatureSpan">(initialURL)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache">
            function <span class="apidocSignatureSpan">simplecrawler.</span>cache
            <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies">
            function <span class="apidocSignatureSpan">simplecrawler.</span>cookies
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawl">
            function <span class="apidocSignatureSpan">simplecrawler.</span>crawl
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler">
            function <span class="apidocSignatureSpan">simplecrawler.</span>crawler
            <span class="apidocSignatureSpan">(initialURL)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue">
            function <span class="apidocSignatureSpan">simplecrawler.</span>queue
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.super_">
            function <span class="apidocSignatureSpan">simplecrawler.</span>super_
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.</span>cache.prototype</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.</span>cookies.prototype</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.</span>crawler.prototype</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.</span>queue.prototype</span>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.cache">module simplecrawler.cache</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.cache">
            function <span class="apidocSignatureSpan">simplecrawler.</span>cache
            <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.Cache">
            function <span class="apidocSignatureSpan">simplecrawler.cache.</span>Cache
            <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.FilesystemBackend">
            function <span class="apidocSignatureSpan">simplecrawler.cache.</span>FilesystemBackend
            <span class="apidocSignatureSpan">(loadParameter)</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.cache.prototype">module simplecrawler.cache.prototype</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.prototype.getCacheData">
            function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>getCacheData
            <span class="apidocSignatureSpan">(queueObject, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.prototype.saveCache">
            function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>saveCache
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cache.prototype.setCacheData">
            function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>setCacheData
            <span class="apidocSignatureSpan">(queueObject, data, callback)</span>
            </a>

        </li>

        <li>

            <span class="apidocSignatureSpan">number <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>_eventsCount</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>_events</span>

        </li>

        <li>

            <span class="apidocSignatureSpan">object <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>domain</span>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.cookies">module simplecrawler.cookies</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.cookies">
            function <span class="apidocSignatureSpan">simplecrawler.</span>cookies
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.Cookie">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.</span>Cookie
            <span class="apidocSignatureSpan">(name, value, expires, path, domain, httponly)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.super_">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.</span>super_
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.cookies.prototype">module simplecrawler.cookies.prototype</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.add">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>add
            <span class="apidocSignatureSpan">(name, value, expiry, path, domain, httponly, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.addFromHeaders">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>addFromHeaders
            <span class="apidocSignatureSpan">(headers, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.get">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>get
            <span class="apidocSignatureSpan">(name, domain, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.getAsHeader">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>getAsHeader
            <span class="apidocSignatureSpan">(domain, path, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.remove">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>remove
            <span class="apidocSignatureSpan">(name, domain, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.cookies.prototype.toString">
            function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>toString
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.crawler">module simplecrawler.crawler</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.crawler">
            function <span class="apidocSignatureSpan">simplecrawler.</span>crawler
            <span class="apidocSignatureSpan">(initialURL)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.cache">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>cache
            <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.crawl">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>crawl
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.queue">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>queue
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.super_">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>super_
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.crawler.prototype">module simplecrawler.crawler.prototype</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.addDownloadCondition">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>addDownloadCondition
            <span class="apidocSignatureSpan">(callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.addFetchCondition">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>addFetchCondition
            <span class="apidocSignatureSpan">(callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.cleanExpandResources">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>cleanExpandResources
            <span class="apidocSignatureSpan">(urlMatch, queueItem)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.crawl">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>crawl
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.decodeBuffer">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>decodeBuffer
            <span class="apidocSignatureSpan">(buffer, contentTypeHeader)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.discoverResources">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>discoverResources
            <span class="apidocSignatureSpan">(resourceText)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.domainValid">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>domainValid
            <span class="apidocSignatureSpan">(host)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.fetchQueueItem">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>fetchQueueItem
            <span class="apidocSignatureSpan">(queueItem)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.getRequestOptions">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>getRequestOptions
            <span class="apidocSignatureSpan">(queueItem)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.getRobotsTxt">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>getRobotsTxt
            <span class="apidocSignatureSpan">(url, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.handleResponse">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>handleResponse
            <span class="apidocSignatureSpan">(queueItem, response, timeCommenced)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.mimeTypeSupported">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>mimeTypeSupported
            <span class="apidocSignatureSpan">(mimetype)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.processURL">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>processURL
            <span class="apidocSignatureSpan">(url, referrer)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.protocolSupported">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>protocolSupported
            <span class="apidocSignatureSpan">(URL)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.queueLinkedItems">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>queueLinkedItems
            <span class="apidocSignatureSpan">(resourceData, queueItem)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.queueURL">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>queueURL
            <span class="apidocSignatureSpan">(url, referrer, force)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.removeDownloadCondition">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>removeDownloadCondition
            <span class="apidocSignatureSpan">(index)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.removeFetchCondition">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>removeFetchCondition
            <span class="apidocSignatureSpan">(index)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.start">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>start
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.stop">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>stop
            <span class="apidocSignatureSpan">(abortRequestsInFlight)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.urlIsAllowed">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>urlIsAllowed
            <span class="apidocSignatureSpan">(url)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.crawler.prototype.wait">
            function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>wait
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.queue">module simplecrawler.queue</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.queue">
            function <span class="apidocSignatureSpan">simplecrawler.</span>queue
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.super_">
            function <span class="apidocSignatureSpan">simplecrawler.queue.</span>super_
            <span class="apidocSignatureSpan">()</span>
            </a>

        </li>

    </ol></li>

    <li class="apidocModuleLi"><a href="#apidoc.module.simplecrawler.queue.prototype">module simplecrawler.queue.prototype</a><ol>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.add">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>add
            <span class="apidocSignatureSpan">(queueItem, force, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.avg">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>avg
            <span class="apidocSignatureSpan">(statisticName, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.countItems">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>countItems
            <span class="apidocSignatureSpan">(comparator, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.defrost">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>defrost
            <span class="apidocSignatureSpan">(filename, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.exists">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>exists
            <span class="apidocSignatureSpan">(url, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.filterItems">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>filterItems
            <span class="apidocSignatureSpan">(comparator, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.freeze">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>freeze
            <span class="apidocSignatureSpan">(filename, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.get">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>get
            <span class="apidocSignatureSpan">(index, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.getLength">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>getLength
            <span class="apidocSignatureSpan">(callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.max">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>max
            <span class="apidocSignatureSpan">(statisticName, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.min">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>min
            <span class="apidocSignatureSpan">(statisticName, callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.oldestUnfetchedItem">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>oldestUnfetchedItem
            <span class="apidocSignatureSpan">(callback)</span>
            </a>

        </li>

        <li>

            <a class="apidocElementLiA" href="#apidoc.element.simplecrawler.queue.prototype.update">
            function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>update
            <span class="apidocSignatureSpan">(id, updates, callback)</span>
            </a>

        </li>

    </ol></li>

</ol></div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler" id="apidoc.module.simplecrawler">module simplecrawler</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.simplecrawler" id="apidoc.element.simplecrawler.simplecrawler">
        function <span class="apidocSignatureSpan"></span>simplecrawler
        <span class="apidocSignatureSpan">(initialURL)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">simplecrawler = function (initialURL) {
    // Allow the crawler to be initialized without the `new` operator. This is
    // handy for chaining API calls
    if (!(this instanceof Crawler)) {
        return new Crawler(initialURL);
    }

    if (arguments.length &gt; 1) {
        throw new Error("Since 1.0.0, simplecrawler takes a single URL when initialized. Protocol, hostname, port and path are inferred
 from that argument.");
    }

    if (typeof initialURL !== "string") {
        throw new Error("The crawler needs a URL string to know where to start crawling");
    }

    EventEmitter.call(this);

    var crawler = this,
        parsedURL = uri(initialURL).normalize();

<span class="apidocCodeCommentSpan">    /**
     * Controls which URL to request first
     * @type {String}
     */
</span>    this.initialURL = initialURL;

    /**
     * Determines what hostname the crawler should limit requests to (so long as
     * {@link Crawler#filterByDomain} is true)
     * @type {String}
     */
    this.host = parsedURL.hostname();

    /**
     * Determines the interval at which new requests are spawned by the crawler,
     * as long as the number of open requests is under the
     * {@link Crawler#maxConcurrency} cap.
     * @type {Number}
     */
    this.interval = 250;

    /**
     * Maximum request concurrency. If necessary, simplecrawler will increase
     * node's http agent maxSockets value to match this setting.
     * @type {Number}
     */
    this.maxConcurrency = 5;

    /**
     * Maximum time we'll wait for headers
     * @type {Number}
     */
    this.timeout = 300000; // 5 minutes

    /**
     * Maximum time we'll wait for async listeners
     * @type {Number}
     */
    this.listenerTTL = 10000; // 10 seconds

    /**
     * Crawler's user agent string
     * @type {String}
     * @default "Node/simplecrawler &lt;version&gt; (https://github.com/cgiffard/node-simplecrawler)"
     */
    this.userAgent =
        "Node/" + packageJson.name + " " + packageJson.version +
        " (" + packageJson.repository.url + ")";

    /**
     * Queue for requests. The crawler can use any implementation so long as it
     * uses the same interface. The default queue is simply backed by an array.
     * @type {FetchQueue}
     */
    this.queue = new FetchQueue();

    /**
     * Controls whether the crawler respects the robots.txt rules of any domain.
     * This is done both with regards to the robots.txt file, and `&lt;meta&gt;` tags
     * that specify a `nofollow` value for robots. The latter only applies if
     * the default {@link Crawler#discoverResources} method is used, though.
     * @type {Boolean}
     */
    this.respectRobotsTxt = true;

    /**
     * Controls whether the crawler is allowed to change the
     * {@link Crawler#host} setting if the first response is a redirect to
     * another domain.
     * @type {Boolean}
     */
    this.allowInitialDomainChange = false;

    /**
     * Controls whether HTTP responses are automatically decompressed based on
     * their Content-Encoding header. If true, it will also assign the
     * appropriate Accept-Encoding header to requests.
     * @type {Boolean}
     */
    this.decompressResponses = true;

    /**
     * Controls whether HTTP responses are automatically character converted to
     * standard JavaScript strings using the {@link https://www.npmjs.com/package/iconv-lite|iconv-lite}
     * module before emitted in the {@link Crawler#event:fetchcomplete} event.
     * The character encoding is interpreted from the Content-Type header
     * firstly, and secondly from any `&lt;meta charset="xxx" /&gt;` tags.
     * @type {Boolean}
     */
    this.decodeResponses = false;

    /**
     * Controls whether the crawler fetches only URL's where the hostname
     * matches {@link Crawler#host}. Unless you want to be crawling the entire
     * internet, I would recommend leaving this on!
     * @type {Boolean}
     */
    this.filterByDomain = true;

    /**
     * Controls whether URL's that points to a subdomain of {@link Crawler#host}
     * should also be fetched.
     * @type {Boolean}
     */
    this.scanSubdomains = fa ...</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cache" id="apidoc.element.simplecrawler.cache">
        function <span class="apidocSignatureSpan">simplecrawler.</span>cache
        <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function Cache(cacheLoadParameter, cacheBackend) {

    // Ensure parameters are how we want them...
    cacheBackend = typeof cacheBackend === "object" ? cacheBackend : FilesystemBackend;
    cacheLoadParameter = cacheLoadParameter instanceof Array ? cacheLoadParameter : [cacheLoadParameter];

    // Now we can just run the factory.
    this.datastore = cacheBackend.apply(cacheBackend, cacheLoadParameter);

    // Instruct the backend to load up.
    this.datastore.load();
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
    default `discoverResources` function is used.*
* `crawler.cache` -
    Specify a cache architecture to use when crawling. Must implement
    `SimpleCache` interface. You can save the site to disk using the built in
    file system cache like this:

    ```js
    crawler.cache = new Crawler.<span class="apidocCodeKeywordSpan">cache</span>('pathToCacheDirectory');
    ```

* `crawler.useProxy=false` -
    The crawler should use an HTTP proxy to make its requests.
* `crawler.proxyHostname="127.0.0.1"` -
    The hostname of the proxy to use for requests.
* `crawler.proxyPort=8123` -
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies" id="apidoc.element.simplecrawler.cookies">
        function <span class="apidocSignatureSpan">simplecrawler.</span>cookies
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">cookies = function () {
    EventEmitter.call(this);

<span class="apidocCodeCommentSpan">    /**
     * The actual jar that holds the cookies
     * @private
     * @type {Array}
     */
</span>    this.cookies = [];
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawl" id="apidoc.element.simplecrawler.crawl">
        function <span class="apidocSignatureSpan">simplecrawler.</span>crawl
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawl = function () {
    throw new Error(
        "Crawler.crawl is deprecated as of version 1.0.0! " +
        "You can now pass a single URL directly to the constructor. " +
        "See the documentation for more details!"
    );
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler" id="apidoc.element.simplecrawler.crawler">
        function <span class="apidocSignatureSpan">simplecrawler.</span>crawler
        <span class="apidocSignatureSpan">(initialURL)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawler = function (initialURL) {
    // Allow the crawler to be initialized without the `new` operator. This is
    // handy for chaining API calls
    if (!(this instanceof Crawler)) {
        return new Crawler(initialURL);
    }

    if (arguments.length &gt; 1) {
        throw new Error("Since 1.0.0, simplecrawler takes a single URL when initialized. Protocol, hostname, port and path are inferred
 from that argument.");
    }

    if (typeof initialURL !== "string") {
        throw new Error("The crawler needs a URL string to know where to start crawling");
    }

    EventEmitter.call(this);

    var crawler = this,
        parsedURL = uri(initialURL).normalize();

<span class="apidocCodeCommentSpan">    /**
     * Controls which URL to request first
     * @type {String}
     */
</span>    this.initialURL = initialURL;

    /**
     * Determines what hostname the crawler should limit requests to (so long as
     * {@link Crawler#filterByDomain} is true)
     * @type {String}
     */
    this.host = parsedURL.hostname();

    /**
     * Determines the interval at which new requests are spawned by the crawler,
     * as long as the number of open requests is under the
     * {@link Crawler#maxConcurrency} cap.
     * @type {Number}
     */
    this.interval = 250;

    /**
     * Maximum request concurrency. If necessary, simplecrawler will increase
     * node's http agent maxSockets value to match this setting.
     * @type {Number}
     */
    this.maxConcurrency = 5;

    /**
     * Maximum time we'll wait for headers
     * @type {Number}
     */
    this.timeout = 300000; // 5 minutes

    /**
     * Maximum time we'll wait for async listeners
     * @type {Number}
     */
    this.listenerTTL = 10000; // 10 seconds

    /**
     * Crawler's user agent string
     * @type {String}
     * @default "Node/simplecrawler &lt;version&gt; (https://github.com/cgiffard/node-simplecrawler)"
     */
    this.userAgent =
        "Node/" + packageJson.name + " " + packageJson.version +
        " (" + packageJson.repository.url + ")";

    /**
     * Queue for requests. The crawler can use any implementation so long as it
     * uses the same interface. The default queue is simply backed by an array.
     * @type {FetchQueue}
     */
    this.queue = new FetchQueue();

    /**
     * Controls whether the crawler respects the robots.txt rules of any domain.
     * This is done both with regards to the robots.txt file, and `&lt;meta&gt;` tags
     * that specify a `nofollow` value for robots. The latter only applies if
     * the default {@link Crawler#discoverResources} method is used, though.
     * @type {Boolean}
     */
    this.respectRobotsTxt = true;

    /**
     * Controls whether the crawler is allowed to change the
     * {@link Crawler#host} setting if the first response is a redirect to
     * another domain.
     * @type {Boolean}
     */
    this.allowInitialDomainChange = false;

    /**
     * Controls whether HTTP responses are automatically decompressed based on
     * their Content-Encoding header. If true, it will also assign the
     * appropriate Accept-Encoding header to requests.
     * @type {Boolean}
     */
    this.decompressResponses = true;

    /**
     * Controls whether HTTP responses are automatically character converted to
     * standard JavaScript strings using the {@link https://www.npmjs.com/package/iconv-lite|iconv-lite}
     * module before emitted in the {@link Crawler#event:fetchcomplete} event.
     * The character encoding is interpreted from the Content-Type header
     * firstly, and secondly from any `&lt;meta charset="xxx" /&gt;` tags.
     * @type {Boolean}
     */
    this.decodeResponses = false;

    /**
     * Controls whether the crawler fetches only URL's where the hostname
     * matches {@link Crawler#host}. Unless you want to be crawling the entire
     * internet, I would recommend leaving this on!
     * @type {Boolean}
     */
    this.filterByDomain = true;

    /**
     * Controls whether URL's that points to a subdomain of {@link Crawler#host}
     * should also be fetched.
     * @type {Boolean}
     */
    this.scanSubdomains = fa ...</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue" id="apidoc.element.simplecrawler.queue">
        function <span class="apidocSignatureSpan">simplecrawler.</span>queue
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">queue = function () {
    Array.call(this);

<span class="apidocCodeCommentSpan">    /**
     * Speeds up {@link FetchQueue.oldestUnfetchedItem} by storing the index at
     * which the latest oldest unfetched queue item was found.
     * @name FetchQueue._oldestUnfetchedIndex
     * @private
     * @type {Number}
     */
</span>    Object.defineProperty(this, "_oldestUnfetchedIndex", {
        enumerable: false,
        writable: true,
        value: 0
    });

    /**
     * Serves as a cache for what URL's have been fetched. Keys are URL's,
     * values are booleans.
     * @name FetchQueue._scanIndex
     * @private
     * @type {Object}
     */
    Object.defineProperty(this, "_scanIndex", {
        enumerable: false,
        writable: true,
        value: {}
    });

    /**
     * Controls what properties can be operated on with the
     * {@link FetchQueue#min}, {@link FetchQueue#avg} and {@link FetchQueue#max}
     * methods.
     * @name FetchQueue._allowedStatistics
     * @type {Array}
     */
    Object.defineProperty(this, "_allowedStatistics", {
        enumerable: false,
        writable: true,
        value: [
            "actualDataSize",
            "contentLength",
            "downloadTime",
            "requestLatency",
            "requestTime"
        ]
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.super_" id="apidoc.element.simplecrawler.super_">
        function <span class="apidocSignatureSpan">simplecrawler.</span>super_
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function EventEmitter() {
  EventEmitter.init.call(this);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>










</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.cache" id="apidoc.module.simplecrawler.cache">module simplecrawler.cache</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.cache.cache" id="apidoc.element.simplecrawler.cache.cache">
        function <span class="apidocSignatureSpan">simplecrawler.</span>cache
        <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function Cache(cacheLoadParameter, cacheBackend) {

    // Ensure parameters are how we want them...
    cacheBackend = typeof cacheBackend === "object" ? cacheBackend : FilesystemBackend;
    cacheLoadParameter = cacheLoadParameter instanceof Array ? cacheLoadParameter : [cacheLoadParameter];

    // Now we can just run the factory.
    this.datastore = cacheBackend.apply(cacheBackend, cacheLoadParameter);

    // Instruct the backend to load up.
    this.datastore.load();
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
    default `discoverResources` function is used.*
* `crawler.cache` -
    Specify a cache architecture to use when crawling. Must implement
    `SimpleCache` interface. You can save the site to disk using the built in
    file system cache like this:

    ```js
    crawler.cache = new Crawler.<span class="apidocCodeKeywordSpan">cache</span>('pathToCacheDirectory');
    ```

* `crawler.useProxy=false` -
    The crawler should use an HTTP proxy to make its requests.
* `crawler.proxyHostname="127.0.0.1"` -
    The hostname of the proxy to use for requests.
* `crawler.proxyPort=8123` -
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cache.Cache" id="apidoc.element.simplecrawler.cache.Cache">
        function <span class="apidocSignatureSpan">simplecrawler.cache.</span>Cache
        <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function Cache(cacheLoadParameter, cacheBackend) {

    // Ensure parameters are how we want them...
    cacheBackend = typeof cacheBackend === "object" ? cacheBackend : FilesystemBackend;
    cacheLoadParameter = cacheLoadParameter instanceof Array ? cacheLoadParameter : [cacheLoadParameter];

    // Now we can just run the factory.
    this.datastore = cacheBackend.apply(cacheBackend, cacheLoadParameter);

    // Instruct the backend to load up.
    this.datastore.load();
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cache.FilesystemBackend" id="apidoc.element.simplecrawler.cache.FilesystemBackend">
        function <span class="apidocSignatureSpan">simplecrawler.cache.</span>FilesystemBackend
        <span class="apidocSignatureSpan">(loadParameter)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function backend(loadParameter) {
    return new FSBackend(loadParameter);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.cache.prototype" id="apidoc.module.simplecrawler.cache.prototype">module simplecrawler.cache.prototype</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.cache.prototype.getCacheData" id="apidoc.element.simplecrawler.cache.prototype.getCacheData">
        function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>getCacheData
        <span class="apidocSignatureSpan">(queueObject, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">getCacheData = function (queueObject, callback) {
    this.datastore.getItem(queueObject, callback);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cache.prototype.saveCache" id="apidoc.element.simplecrawler.cache.prototype.saveCache">
        function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>saveCache
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">saveCache = function () {
    this.datastore.saveCache();
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cache.prototype.setCacheData" id="apidoc.element.simplecrawler.cache.prototype.setCacheData">
        function <span class="apidocSignatureSpan">simplecrawler.cache.prototype.</span>setCacheData
        <span class="apidocSignatureSpan">(queueObject, data, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">setCacheData = function (queueObject, data, callback) {
    this.datastore.setItem(queueObject, data, callback);
    this.emit("setcache", queueObject, data);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>








</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.cookies" id="apidoc.module.simplecrawler.cookies">module simplecrawler.cookies</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.cookies" id="apidoc.element.simplecrawler.cookies.cookies">
        function <span class="apidocSignatureSpan">simplecrawler.</span>cookies
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">cookies = function () {
    EventEmitter.call(this);

<span class="apidocCodeCommentSpan">    /**
     * The actual jar that holds the cookies
     * @private
     * @type {Array}
     */
</span>    this.cookies = [];
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.Cookie" id="apidoc.element.simplecrawler.cookies.Cookie">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.</span>Cookie
        <span class="apidocSignatureSpan">(name, value, expires, path, domain, httponly)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">Cookie = function (name, value, expires, path, domain, httponly) {
    if (!name) {
        throw new Error("A name is required to create a cookie.");
    }

    // Parse date to timestamp - consider it never expiring if timestamp is not
    // passed to the function
    if (expires) {

        if (typeof expires !== "number") {
            expires = (new Date(expires)).getTime();
        }

    } else {
        expires = -1;
    }

    this.name = name;
    this.value = value || "";
    this.expires = expires;
    this.path = path || "/";
    this.domain = domain || "*";
    this.httponly = Boolean(httponly);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.super_" id="apidoc.element.simplecrawler.cookies.super_">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.</span>super_
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function EventEmitter() {
  EventEmitter.init.call(this);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.cookies.prototype" id="apidoc.module.simplecrawler.cookies.prototype">module simplecrawler.cookies.prototype</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.add" id="apidoc.element.simplecrawler.cookies.prototype.add">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>add
        <span class="apidocSignatureSpan">(name, value, expiry, path, domain, httponly, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">add = function (name, value, expiry, path, domain, httponly, callback) {
    var existingIndex = -1, newCookie;

    if (arguments.length &gt; 1) {
        newCookie = new Cookie(name, value, expiry, path, domain, httponly);
    } else if (name instanceof Cookie) {
        newCookie = name;
    } else {
        newCookie = Cookie.fromString(name);
    }

    // Are we updating an existing cookie or adding a new one?
    this.cookies.forEach(function(cookie, index) {
        if (cookie.name === newCookie.name &amp;&amp; cookie.matchDomain(newCookie.domain)) {
            existingIndex = index;
        }
    });

    if (existingIndex === -1) {
        this.cookies.push(newCookie);
    } else {
        this.cookies[existingIndex] = newCookie;
    }

<span class="apidocCodeCommentSpan">    /**
     * Fired when a cookie has been added to the jar
     * @event CookieJar#addcookie
     * @param {Cookie} cookie The cookie that has been added
     */
</span>    this.emit("addcookie", newCookie);

    if (callback instanceof Function) {
        callback(null, newCookie);
    }

    return this;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
var jar = this;

if (!Array.isArray(headers)) {
    headers = [headers];
}

headers.forEach(function(header) {
    jar.<span class="apidocCodeKeywordSpan">add</span>(header);
});

if (callback instanceof Function) {
    callback(null);
}

return jar;
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.addFromHeaders" id="apidoc.element.simplecrawler.cookies.prototype.addFromHeaders">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>addFromHeaders
        <span class="apidocSignatureSpan">(headers, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">addFromHeaders = function (headers, callback) {
    var jar = this;

    if (!Array.isArray(headers)) {
        headers = [headers];
    }

    headers.forEach(function(header) {
        jar.add(header);
    });

    if (callback instanceof Function) {
        callback(null);
    }

    return jar;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
// request and the next
jar: true
    }, function (error, response, body) {
// Start by saving the cookies. We'll likely be assigned a session cookie
// straight off the bat, and then the server will remember the fact that
// this session is logged in as user "iamauser" after we've successfully
// logged in
crawler.cookies.<span class="apidocCodeKeywordSpan">addFromHeaders</span>(response.headers["set-cookie"]);

// We want to get the names and values of all relevant inputs on the page,
// so that any CSRF tokens or similar things are included in the POST
// request
var $ = cheerio.load(body),
    formDefaults = {},
    // You should adapt these selectors so that they target the
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.get" id="apidoc.element.simplecrawler.cookies.prototype.get">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>get
        <span class="apidocSignatureSpan">(name, domain, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">get = function (name, domain, callback) {
    var cookies = this.cookies.filter(function(cookie) {
        // If the names don't match, we're not returning this cookie
        if (Boolean(name) &amp;&amp; cookie.name !== name) {
            return false;
        }

        // If the domains don't match, we're not returning this cookie
        if (Boolean(domain) &amp;&amp; !cookie.matchDomain(domain)) {
            return false;
        }

        return true;
    });

    if (callback instanceof Function) {
        callback(null, cookies);
    }

    return cookies;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
Like any other web crawler, simplecrawler has a queue. It can be directly
accessed through `crawler.queue` and implements an asynchronous interface for
accessing queue items and statistics. There are several methods for interacting
with the queue, the simplest being `crawler.queue.get`, which lets you get a
queue item at a specific index in the queue.

```js
crawler.queue.<span class="apidocCodeKeywordSpan">get</span>(5, function (queueItem) {
    // Do something with the queueItem
});
```

*All queue method are in reality synchronous by default, but simplecrawler is
built to be able to use different queues that implement the same interface, and
those implementations can be asynchronous - which means they could eg. be backed
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.getAsHeader" id="apidoc.element.simplecrawler.cookies.prototype.getAsHeader">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>getAsHeader
        <span class="apidocSignatureSpan">(domain, path, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">getAsHeader = function (domain, path, callback) {
    var headers = this.cookies.filter(function(cookie) {
        if (cookie.isExpired()) {
            return false;
        }
        if (!domain &amp;&amp; !path) {
            return true;
        }
        if (domain) {
            return cookie.matchDomain(domain);
        }
        if (path) {
            return cookie.matchPath(path);
        }
    })
    .map(function(cookie) {
        return cookie.toString();
    });

    if (callback instanceof Function) {
        callback(null, headers);
    }

    return headers;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
};

/**
* Generates a newline-separated list of all cookies in the jar
* @return {String} Returns stringified versions of all cookies in the jar in a newline separated string
*/
CookieJar.prototype.toString = function() {
   return this.<span class="apidocCodeKeywordSpan">getAsHeader</span>().join("\n");
};


/**
* Creates a new cookies
* @class
* @param {String} name                       Name of the new cookie
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.remove" id="apidoc.element.simplecrawler.cookies.prototype.remove">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>remove
        <span class="apidocSignatureSpan">(name, domain, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">remove = function (name, domain, callback) {
    var cookiesRemoved = [],
        jar = this;

    jar.cookies.forEach(function(cookie, index) {
        // If the names don't match, we're not removing this cookie
        if (Boolean(name) &amp;&amp; cookie.name !== name) {
            return false;
        }

        // If the domains don't match, we're not removing this cookie
        if (Boolean(domain) &amp;&amp; !cookie.matchDomain(domain)) {
            return false;
        }

        // Matched. Remove!
        cookiesRemoved.push(jar.cookies.splice(index, 1));
    });

<span class="apidocCodeCommentSpan">    /**
     * Fired when one or multiple cookie have been removed from the jar
     * @event CookieJar#removecookie
     * @param {Cookie[]} cookie The cookies that have been removed
     */
</span>    jar.emit("removecookie", cookiesRemoved);

    if (callback instanceof Function) {
        callback(null, cookiesRemoved);
    }

    return cookiesRemoved;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.cookies.prototype.toString" id="apidoc.element.simplecrawler.cookies.prototype.toString">
        function <span class="apidocSignatureSpan">simplecrawler.cookies.prototype.</span>toString
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">toString = function () {
    return this.getAsHeader().join("\n");
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
browser, like phantomJS.

The example below demonstrates how one might achieve basic HTML-correct
discovery of only link tags using cheerio.

```js
crawler.discoverResources = function(buffer, queueItem) {
    var $ = cheerio.load(buffer.<span class="apidocCodeKeywordSpan">toString</span>("utf8"));

    return $("a[href]").map(function () {
        return $(this).attr("href");
    }).get();
};
```
...</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.crawler" id="apidoc.module.simplecrawler.crawler">module simplecrawler.crawler</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.crawler" id="apidoc.element.simplecrawler.crawler.crawler">
        function <span class="apidocSignatureSpan">simplecrawler.</span>crawler
        <span class="apidocSignatureSpan">(initialURL)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawler = function (initialURL) {
    // Allow the crawler to be initialized without the `new` operator. This is
    // handy for chaining API calls
    if (!(this instanceof Crawler)) {
        return new Crawler(initialURL);
    }

    if (arguments.length &gt; 1) {
        throw new Error("Since 1.0.0, simplecrawler takes a single URL when initialized. Protocol, hostname, port and path are inferred
 from that argument.");
    }

    if (typeof initialURL !== "string") {
        throw new Error("The crawler needs a URL string to know where to start crawling");
    }

    EventEmitter.call(this);

    var crawler = this,
        parsedURL = uri(initialURL).normalize();

<span class="apidocCodeCommentSpan">    /**
     * Controls which URL to request first
     * @type {String}
     */
</span>    this.initialURL = initialURL;

    /**
     * Determines what hostname the crawler should limit requests to (so long as
     * {@link Crawler#filterByDomain} is true)
     * @type {String}
     */
    this.host = parsedURL.hostname();

    /**
     * Determines the interval at which new requests are spawned by the crawler,
     * as long as the number of open requests is under the
     * {@link Crawler#maxConcurrency} cap.
     * @type {Number}
     */
    this.interval = 250;

    /**
     * Maximum request concurrency. If necessary, simplecrawler will increase
     * node's http agent maxSockets value to match this setting.
     * @type {Number}
     */
    this.maxConcurrency = 5;

    /**
     * Maximum time we'll wait for headers
     * @type {Number}
     */
    this.timeout = 300000; // 5 minutes

    /**
     * Maximum time we'll wait for async listeners
     * @type {Number}
     */
    this.listenerTTL = 10000; // 10 seconds

    /**
     * Crawler's user agent string
     * @type {String}
     * @default "Node/simplecrawler &lt;version&gt; (https://github.com/cgiffard/node-simplecrawler)"
     */
    this.userAgent =
        "Node/" + packageJson.name + " " + packageJson.version +
        " (" + packageJson.repository.url + ")";

    /**
     * Queue for requests. The crawler can use any implementation so long as it
     * uses the same interface. The default queue is simply backed by an array.
     * @type {FetchQueue}
     */
    this.queue = new FetchQueue();

    /**
     * Controls whether the crawler respects the robots.txt rules of any domain.
     * This is done both with regards to the robots.txt file, and `&lt;meta&gt;` tags
     * that specify a `nofollow` value for robots. The latter only applies if
     * the default {@link Crawler#discoverResources} method is used, though.
     * @type {Boolean}
     */
    this.respectRobotsTxt = true;

    /**
     * Controls whether the crawler is allowed to change the
     * {@link Crawler#host} setting if the first response is a redirect to
     * another domain.
     * @type {Boolean}
     */
    this.allowInitialDomainChange = false;

    /**
     * Controls whether HTTP responses are automatically decompressed based on
     * their Content-Encoding header. If true, it will also assign the
     * appropriate Accept-Encoding header to requests.
     * @type {Boolean}
     */
    this.decompressResponses = true;

    /**
     * Controls whether HTTP responses are automatically character converted to
     * standard JavaScript strings using the {@link https://www.npmjs.com/package/iconv-lite|iconv-lite}
     * module before emitted in the {@link Crawler#event:fetchcomplete} event.
     * The character encoding is interpreted from the Content-Type header
     * firstly, and secondly from any `&lt;meta charset="xxx" /&gt;` tags.
     * @type {Boolean}
     */
    this.decodeResponses = false;

    /**
     * Controls whether the crawler fetches only URL's where the hostname
     * matches {@link Crawler#host}. Unless you want to be crawling the entire
     * internet, I would recommend leaving this on!
     * @type {Boolean}
     */
    this.filterByDomain = true;

    /**
     * Controls whether URL's that points to a subdomain of {@link Crawler#host}
     * should also be fetched.
     * @type {Boolean}
     */
    this.scanSubdomains = fa ...</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.cache" id="apidoc.element.simplecrawler.crawler.cache">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>cache
        <span class="apidocSignatureSpan">(cacheLoadParameter, cacheBackend)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function Cache(cacheLoadParameter, cacheBackend) {

    // Ensure parameters are how we want them...
    cacheBackend = typeof cacheBackend === "object" ? cacheBackend : FilesystemBackend;
    cacheLoadParameter = cacheLoadParameter instanceof Array ? cacheLoadParameter : [cacheLoadParameter];

    // Now we can just run the factory.
    this.datastore = cacheBackend.apply(cacheBackend, cacheLoadParameter);

    // Instruct the backend to load up.
    this.datastore.load();
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
    default `discoverResources` function is used.*
* `crawler.cache` -
    Specify a cache architecture to use when crawling. Must implement
    `SimpleCache` interface. You can save the site to disk using the built in
    file system cache like this:

    ```js
    crawler.cache = new Crawler.<span class="apidocCodeKeywordSpan">cache</span>('pathToCacheDirectory');
    ```

* `crawler.useProxy=false` -
    The crawler should use an HTTP proxy to make its requests.
* `crawler.proxyHostname="127.0.0.1"` -
    The hostname of the proxy to use for requests.
* `crawler.proxyPort=8123` -
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.crawl" id="apidoc.element.simplecrawler.crawler.crawl">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>crawl
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawl = function () {
    throw new Error(
        "Crawler.crawl is deprecated as of version 1.0.0! " +
        "You can now pass a single URL directly to the constructor. " +
        "See the documentation for more details!"
    );
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.queue" id="apidoc.element.simplecrawler.crawler.queue">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>queue
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">queue = function () {
    Array.call(this);

<span class="apidocCodeCommentSpan">    /**
     * Speeds up {@link FetchQueue.oldestUnfetchedItem} by storing the index at
     * which the latest oldest unfetched queue item was found.
     * @name FetchQueue._oldestUnfetchedIndex
     * @private
     * @type {Number}
     */
</span>    Object.defineProperty(this, "_oldestUnfetchedIndex", {
        enumerable: false,
        writable: true,
        value: 0
    });

    /**
     * Serves as a cache for what URL's have been fetched. Keys are URL's,
     * values are booleans.
     * @name FetchQueue._scanIndex
     * @private
     * @type {Object}
     */
    Object.defineProperty(this, "_scanIndex", {
        enumerable: false,
        writable: true,
        value: {}
    });

    /**
     * Controls what properties can be operated on with the
     * {@link FetchQueue#min}, {@link FetchQueue#avg} and {@link FetchQueue#max}
     * methods.
     * @name FetchQueue._allowedStatistics
     * @type {Array}
     */
    Object.defineProperty(this, "_allowedStatistics", {
        enumerable: false,
        writable: true,
        value: [
            "actualDataSize",
            "contentLength",
            "downloadTime",
            "requestLatency",
            "requestTime"
        ]
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.super_" id="apidoc.element.simplecrawler.crawler.super_">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.</span>super_
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function EventEmitter() {
  EventEmitter.init.call(this);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.crawler.prototype" id="apidoc.module.simplecrawler.crawler.prototype">module simplecrawler.crawler.prototype</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.addDownloadCondition" id="apidoc.element.simplecrawler.crawler.prototype.addDownloadCondition">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>addDownloadCondition
        <span class="apidocSignatureSpan">(callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">addDownloadCondition = function (callback) {
    if (!(callback instanceof Function)) {
        throw new Error("Download condition must be a function");
    }

    this._downloadConditions.push(callback);
    return this._downloadConditions.length - 1;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
### Adding a download condition

Download conditions are added in much the same way as fetch conditions, with the
`crawler.addDownloadCondition` method. This method returns an ID that can be
used to remove the condition later.

```js
var conditionID = myCrawler.<span class="apidocCodeKeywordSpan">addDownloadCondition</span>(function(queueItem, response, callback
) {
    callback(null,
        queueItem.stateData.contentType === "image/png" &amp;&amp;
        queueItem.stateData.contentLength &lt; 5 * 1000 * 1000
    );
});
```
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.addFetchCondition" id="apidoc.element.simplecrawler.crawler.prototype.addFetchCondition">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>addFetchCondition
        <span class="apidocSignatureSpan">(callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">addFetchCondition = function (callback) {
    if (!(callback instanceof Function)) {
        throw new Error("Fetch condition must be a function");
    }

    this._fetchConditions.push(callback);
    return this._fetchConditions.length - 1;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...

This example fetch condition prevents URL's ending in `.pdf` from being
downloaded. Adding a fetch condition assigns it an ID, which the
`addFetchCondition` function returns. You can use this ID to remove the
condition later.

```js
var conditionID = myCrawler.<span class="apidocCodeKeywordSpan">addFetchCondition</span>(function(queueItem, referrerQueueItem,
callback) {
    callback(null, !queueItem.path.match(/\.pdf$/i));
});
```

Fetch conditions are called with three arguments: `queueItem`,
`referrerQueueItem` and `callback`. `queueItem` represents the resource to be
fetched (or not), and `referrerQueueItem` represents the resource where the new
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.cleanExpandResources" id="apidoc.element.simplecrawler.crawler.prototype.cleanExpandResources">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>cleanExpandResources
        <span class="apidocSignatureSpan">(urlMatch, queueItem)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">cleanExpandResources = function (urlMatch, queueItem) {
    var crawler = this;

    if (!urlMatch) {
        return [];
    }

    return urlMatch
        .filter(Boolean)
        .map(function(url) {
            return cleanURL(url, queueItem);
        })
        .reduce(function(list, URL) {

            // Ensure URL is whole and complete
            try {
                URL = uri(URL)
                    .absoluteTo(queueItem.url || "")
                    .normalize()
                    .href();
            } catch (e) {
                // But if URI.js couldn't parse it - nobody can!
                return list;
            }

            // If we hit an empty item, don't return it
            if (!URL.length) {
                return list;
            }

            // If we don't support the protocol in question
            if (!crawler.protocolSupported(URL)) {
                return list;
            }

            // Does the item already exist in the list?
            var exists = list.some(function(entry) {
                return entry === URL;
            });

            if (exists) {
                return list;
            }

            return list.concat(URL);
        }, []);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.crawl" id="apidoc.element.simplecrawler.crawler.prototype.crawl">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>crawl
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">crawl = function () {
    var crawler = this;

    if (crawler._openRequests.length &gt;= crawler.maxConcurrency ||
        crawler.fetchingRobotsTxt) {
        return crawler;
    }

    crawler.queue.oldestUnfetchedItem(function(error, queueItem) {
        if (error) {
            // Do nothing
        } else if (queueItem) {

            var url = uri(queueItem.url).normalize();
            var host = uri({
                protocol: url.protocol(),
                hostname: url.hostname(),
                port: url.port()
            }).href();

            if (crawler.respectRobotsTxt &amp;&amp; crawler._touchedHosts.indexOf(host) === -1) {
                crawler._touchedHosts.push(host);
                crawler.fetchingRobotsTxt = true;

                var robotsTxtUrl = uri(host).pathname("/robots.txt").href();

                crawler.getRobotsTxt(robotsTxtUrl, function(error, robotsTxtUrl, robotsTxtBody) {
                    if (error) {
<span class="apidocCodeCommentSpan">                        /**
                         * Fired when an error was encountered while retrieving a robots.txt file
                         * @event Crawler#robotstxterror
                         * @param {Error} error The error returned from {@link Crawler#getRobotsTxt}
                         */
</span>                        crawler.emit("robotstxterror", error);
                    } else {
                        crawler._robotsTxts.push(robotsTxtParser(robotsTxtUrl, robotsTxtBody));
                    }

                    crawler.fetchingRobotsTxt = false;

                    // It could be that the first URL we queued for any particular
                    // host is in fact disallowed, so we double check once we've
                    // fetched the robots.txt
                    if (crawler.urlIsAllowed(queueItem.url)) {
                        crawler.fetchQueueItem(queueItem);
                    } else {
                        crawler.queue.update(queueItem.id, {
                            fetched: true,
                            status: "disallowed"
                        }, function(error, queueItem) {
                            crawler.emit("fetchdisallowed", queueItem);
                        });
                    }
                });
            } else {

                crawler.fetchQueueItem(queueItem);
            }
        } else if (!crawler._openRequests.length &amp;&amp; !crawler._openListeners) {

            crawler.queue.countItems({ fetched: true }, function(err, completeCount) {
                if (err) {
                    throw err;
                }

                crawler.queue.getLength(function(err, length) {
                    if (err) {
                        throw err;
                    }

                    if (completeCount === length) {
                        /**
                         * Fired when the crawl has completed - all resources in the queue have been dealt with
                         * @event Crawler#complete
                         */
                        crawler.emit("complete");
                        crawler.stop();
                    }
                });
            });
        }
    });

    return crawler;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.decodeBuffer" id="apidoc.element.simplecrawler.crawler.prototype.decodeBuffer">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>decodeBuffer
        <span class="apidocSignatureSpan">(buffer, contentTypeHeader)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">decodeBuffer = function (buffer, contentTypeHeader) {
    contentTypeHeader = contentTypeHeader || "";

    var embeddedEncoding = /&lt;meta[^&gt;]*charset\s*=\s*["']?([\w\-]*)/i.exec(buffer.toString(undefined, 0, 512)) || [],
        encoding = contentTypeHeader.split("charset=")[1] || embeddedEncoding[1] || contentTypeHeader;

    encoding = iconv.encodingExists(encoding) ? encoding : "utf8";

    return iconv.decode(buffer, encoding);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.discoverResources" id="apidoc.element.simplecrawler.crawler.prototype.discoverResources">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>discoverResources
        <span class="apidocSignatureSpan">(resourceText)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">discoverResources = function (resourceText) {
    var crawler = this;

    if (!crawler.parseHTMLComments) {
        resourceText = resourceText.replace(/&lt;!--([\s\S]+?)--&gt;/g, "");
    }

    if (!crawler.parseScriptTags) {
        resourceText = resourceText.replace(/&lt;script(.*?)&gt;([\s\S]*?)&lt;\/script&gt;/gi, "");
    }

    if (crawler.respectRobotsTxt &amp;&amp; /&lt;meta(?:\s[^&gt;]*)?\sname\s*=\s*["']?robots["']?[^&gt;]*&gt;/i.test(resourceText)) {
        var robotsValue = /&lt;meta(?:\s[^&gt;]*)?\svalue\s*=\s*["']?([\w\s,]+)["']?[^&gt;]*&gt;/i.exec(resourceText.toLowerCase());

        if (Array.isArray(robotsValue) &amp;&amp; /nofollow/i.test(robotsValue[1])) {
            return [];
        }
    }

    // Rough scan for URLs
    return crawler.discoverRegex.reduce(function(list, extracter) {
        var resources;

        if (extracter instanceof Function) {
            resources = extracter(resourceText);
        } else {
            resources = resourceText.match(extracter);
        }

        return resources ? list.concat(resources) : list;
    }, []);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.domainValid" id="apidoc.element.simplecrawler.crawler.prototype.domainValid">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>domainValid
        <span class="apidocSignatureSpan">(host)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">domainValid = function (host) {
    var crawler = this;

    // If we're ignoring the WWW domain, remove the WWW for comparisons...
    if (crawler.ignoreWWWDomain) {
        host = host.replace(/^www\./i, "");
    }

    function domainInWhitelist(host) {

        // If there's no whitelist, or the whitelist is of zero length,
        // just return false.
        if (!crawler.domainWhitelist || !crawler.domainWhitelist.length) {
            return false;
        }

        // Otherwise, scan through it.
        return crawler.domainWhitelist.some(function(entry) {
            // If the domain is just equal, return true.
            if (host === entry) {
                return true;
            }
            // If we're ignoring WWW subdomains, and both domains,
            // less www. are the same, return true.
            if (crawler.ignoreWWWDomain &amp;&amp; host === entry.replace(/^www\./i, "")) {
                return true;
            }
            return false;
        });
    }

    // Checks if the first domain is a subdomain of the second
    function isSubdomainOf(subdomain, host) {

        // Comparisons must be case-insensitive
        subdomain   = subdomain.toLowerCase();
        host        = host.toLowerCase();

        // If we're ignoring www, remove it from both
        // (if www is the first domain component...)
        if (crawler.ignoreWWWDomain) {
            subdomain = subdomain.replace(/^www./ig, "");
            host = host.replace(/^www./ig, "");
        }

        // They should be the same flipped around!
        return subdomain.split("").reverse().join("").substr(0, host.length) ===
                host.split("").reverse().join("");
    }

           // If we're not filtering by domain, just return true.
    return !crawler.filterByDomain ||
           // Or if the domain is just the right one, return true.
           host === crawler.host ||
           // Or if we're ignoring WWW subdomains, and both domains,
           // less www. are the same, return true.
           crawler.ignoreWWWDomain &amp;&amp;
               crawler.host.replace(/^www\./i, "") ===
                   host.replace(/^www\./i, "") ||
           // Or if the domain in question exists in the domain whitelist,
           // return true.
           domainInWhitelist(host) ||
           // Or if we're scanning subdomains, and this domain is a subdomain
           // of the crawler's set domain, return true.
           crawler.scanSubdomains &amp;&amp; isSubdomainOf(host, crawler.host);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.fetchQueueItem" id="apidoc.element.simplecrawler.crawler.prototype.fetchQueueItem">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>fetchQueueItem
        <span class="apidocSignatureSpan">(queueItem)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">fetchQueueItem = function (queueItem) {
    var crawler = this;

    crawler.queue.update(queueItem.id, {
        status: "spooled"
    }, function(error, queueItem) {
        if (error) {
            return crawler.emit("queueerror", error, queueItem);
        }

        var client = queueItem.protocol === "https" ? https : http,
            agent  = queueItem.protocol === "https" ? crawler.httpsAgent : crawler.httpAgent;

        if (agent.maxSockets &lt; crawler.maxConcurrency) {
            agent.maxSockets = crawler.maxConcurrency;
        }

        if (client === https &amp;&amp; crawler.ignoreInvalidSSL === true) {
            client.rejectUnauthorized = false;
            client.strictSSL = false;
        }

        var requestOptions = crawler.getRequestOptions(queueItem),
            timeCommenced = Date.now();

        var clientRequest = client.request(requestOptions, function(response) {
            crawler.handleResponse(queueItem, response, timeCommenced);
        });

        clientRequest.end();

        // Enable central tracking of this request
        crawler._openRequests.push(clientRequest);

        // Ensure the request is removed from the tracking array if it is
        // forcibly aborted
        clientRequest.on("abort", function() {
            if (crawler._openRequests.indexOf(clientRequest) &gt; -1) {
                crawler._openRequests.splice(
                    crawler._openRequests.indexOf(clientRequest), 1);
            }
        });

        clientRequest.setTimeout(crawler.timeout, function() {
            if (queueItem.fetched) {
                return;
            }

            if (crawler.running &amp;&amp; !queueItem.fetched) {
                // Remove this request from the open request map
                crawler._openRequests.splice(
                    crawler._openRequests.indexOf(clientRequest), 1);
            }

            crawler.queue.update(queueItem.id, {
                fetched: true,
                status: "timeout"
            }, function(error, queueItem) {
                if (error) {
                    return crawler.emit("queueerror", error, queueItem);
                }

<span class="apidocCodeCommentSpan">                /**
                 * Fired when a request times out
                 * @event Crawler#fetchtimeout
                 * @param {QueueItem} queueItem The queue item for which the request timed out
                 * @param {Number} timeout      The delay in milliseconds after which the request timed out
                 */
</span>                crawler.emit("fetchtimeout", queueItem, crawler.timeout);
                clientRequest.abort();
            });
        });

        clientRequest.on("error", function(errorData) {

            // This event will be thrown if we manually aborted the request,
            // but we don't want to do anything in that case.
            if (clientRequest.aborted) {
                return;
            }

            if (crawler.running &amp;&amp; !queueItem.fetched) {
                // Remove this request from the open request map
                crawler._openRequests.splice(
                    crawler._openRequests.indexOf(clientRequest), 1);
            }

            crawler.queue.update(queueItem.id, {
                fetched: true,
                status: "failed",
                stateData: {
                    code: 600
                }
            }, function(error, queueItem) {
                if (error) {
                    return crawler.emit("queueerror", error, queueItem);
                }

                /**
                 * Fired when a request encounters an unknown error
                 * @event Crawler#fetchclienterror
                 * @param {QueueItem} queueItem The queue item for which the request has errored
                 * @param {Object} error        The error supplied to the `error` event on the request
                 */
                crawler.emit("fetchclienterror", queueItem, errorData);
            });
        });

        /**
         * Fired just after a request has been initiated
         * @event Crawler#fetchstart
         * @param {QueueIt ...</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.getRequestOptions" id="apidoc.element.simplecrawler.crawler.prototype.getRequestOptions">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>getRequestOptions
        <span class="apidocSignatureSpan">(queueItem)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">getRequestOptions = function (queueItem) {
    var crawler = this;

    var agent = queueItem.protocol === "https" ? crawler.httpsAgent : crawler.httpAgent;

    // Extract request options from queue;
    var requestHost = queueItem.host,
        requestPort = queueItem.port,
        requestPath = queueItem.path;

    // Are we passing through an HTTP proxy?
    if (crawler.useProxy) {
        requestHost = crawler.proxyHostname;
        requestPort = crawler.proxyPort;
        requestPath = queueItem.url;
    }

    var isStandardHTTPPort = queueItem.protocol === "http" &amp;&amp; queueItem.port !== 80,
        isStandardHTTPSPort = queueItem.protocol === "https" &amp;&amp; queueItem.port !== 443,
        isStandardPort = isStandardHTTPPort || isStandardHTTPSPort;

    // Load in request options
    var requestOptions = {
        method: "GET",
        host: requestHost,
        port: requestPort,
        path: requestPath,
        agent: agent,
        headers: {
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "User-Agent": crawler.userAgent,
            "Host": queueItem.host + (queueItem.port &amp;&amp; isStandardPort ? ":" + queueItem.port : "")
        }
    };

    if (crawler.decompressResponses) {
        requestOptions.headers["Accept-Encoding"] = "gzip, deflate";
    }

    if (queueItem.referrer) {
        requestOptions.headers.Referer = queueItem.referrer;
    }

    // If port is one of the HTTP/HTTPS defaults, delete the option to avoid conflicts
    if (requestPort === 80 || requestPort === 443 || !requestPort) {
        delete requestOptions.port;
    }

    // Add cookie header from cookie jar if we're configured to
    // send/accept cookies
    if (crawler.acceptCookies &amp;&amp; crawler.cookies.getAsHeader()) {
        requestOptions.headers.cookie =
            crawler.cookies.getAsHeader(queueItem.host, queueItem.path);
    }

    // Add auth headers if we need them
    if (crawler.needsAuth) {
        var auth = crawler.authUser + ":" + crawler.authPass;

        // Generate auth header
        auth = "Basic " + new Buffer(auth).toString("base64");
        requestOptions.headers.Authorization = auth;
    }

    // Add proxy auth if we need it
    if (crawler.proxyUser !== null &amp;&amp; crawler.proxyPass !== null) {
        var proxyAuth = crawler.proxyUser + ":" + crawler.proxyPass;

        // Generate auth header
        proxyAuth = "Basic " + new Buffer(proxyAuth).toString("base64");
        requestOptions.headers["Proxy-Authorization"] = proxyAuth;
    }

    // And if we've got any custom headers available
    if (crawler.customHeaders) {
        for (var header in crawler.customHeaders) {
            if (crawler.customHeaders.hasOwnProperty(header)) {
                requestOptions.headers[header] = crawler.customHeaders[header];
            }
        }
    }

    return requestOptions;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.getRobotsTxt" id="apidoc.element.simplecrawler.crawler.prototype.getRobotsTxt">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>getRobotsTxt
        <span class="apidocSignatureSpan">(url, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">getRobotsTxt = function (url, callback) {
    var crawler = this,
        errorMsg;

    var robotsTxtUrl = uri(url);
    var client = robotsTxtUrl.protocol() === "https" ? https : http;

    // Apply the ignoreInvalidSSL setting to https connections
    if (client === https &amp;&amp; crawler.ignoreInvalidSSL === true) {
        client.rejectUnauthorized = false;
        client.strictSSL = false;
    }

    var requestOptions = crawler.getRequestOptions(crawler.processURL(robotsTxtUrl.href()));

    // Get the resource!
    var clientRequest = client.request(requestOptions, function(response) {
        if (response.statusCode &gt;= 200 &amp;&amp; response.statusCode &lt; 300) {
            var responseLength =
                    parseInt(response.headers["content-length"], 10) ||
                    crawler.maxResourceSize,
                responseBuffer = new Buffer(responseLength),
                responseLengthReceived = 0;

            response.on("data", function(chunk) {
                if (responseLengthReceived + chunk.length &lt;= crawler.maxResourceSize) {
                    chunk.copy(responseBuffer, responseLengthReceived, 0, chunk.length);
                    responseLengthReceived += chunk.length;
                } else {
                    response.destroy();
                    callback(new Error("robots.txt exceeded maxResourceSize"));
                }
            });

            var decodeAndReturnResponse = function(error, responseBuffer) {
                if (error) {
                    return callback(new Error("Couldn't unzip robots.txt response body"));
                }

                var contentType = response.headers["content-type"],
                    responseBody = crawler.decodeBuffer(responseBuffer, contentType);

                callback(undefined, robotsTxtUrl.href(), responseBody);
            };

            response.on("end", function() {
                var contentEncoding = response.headers["content-encoding"];

                if (contentEncoding &amp;&amp; /(gzip|deflate)/.test(contentEncoding)) {
                    zlib.unzip(responseBuffer, decodeAndReturnResponse);
                } else {
                    decodeAndReturnResponse(undefined, responseBuffer);
                }
            });
        } else if (response.statusCode &gt;= 300 &amp;&amp; response.statusCode &lt; 400 &amp;&amp;
            response.headers.location) {

            response.destroy();

            var redirectTarget = uri(response.headers.location)
                .absoluteTo(robotsTxtUrl)
                .normalize();

            if (crawler.domainValid(redirectTarget.hostname())) {
                crawler.getRobotsTxt(redirectTarget.href(), callback);
            } else {
                errorMsg = util.format("%s redirected to a disallowed domain (%s)", robotsTxtUrl.href(), redirectTarget.hostname
());
                callback(new Error(errorMsg));
            }
        } else {
            response.destroy();

            errorMsg = util.format("Server responded with status %d when fetching robots.txt", response.statusCode);
            callback(new Error(errorMsg));
        }
    });

    clientRequest.end();

    clientRequest.setTimeout(crawler.timeout, function() {
        clientRequest.abort();
        callback(new Error("robots.txt request timed out"));
    });

    clientRequest.on("error", function(errorData) {
        if (!clientRequest.aborted) {
            callback(errorData);
        }
    });

    return crawler;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.handleResponse" id="apidoc.element.simplecrawler.crawler.prototype.handleResponse">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>handleResponse
        <span class="apidocSignatureSpan">(queueItem, response, timeCommenced)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">handleResponse = function (queueItem, response, timeCommenced) {
    var crawler = this,
        dataReceived = false,
        timeHeadersReceived = Date.now(),
        timeDataReceived,
        redirectQueueItem,
        responseBuffer,
        responseLength,
        responseLengthReceived = 0,
        contentType = response.headers["content-type"];

    timeCommenced = timeCommenced || Date.now();
    responseLength = parseInt(response.headers["content-length"], 10);
    responseLength = !isNaN(responseLength) ? responseLength : 0;

    crawler.queue.update(queueItem.id, {
        stateData: {
            requestLatency: timeHeadersReceived - timeCommenced,
            requestTime: timeHeadersReceived - timeCommenced,
            contentLength: responseLength,
            contentType: contentType,
            code: response.statusCode,
            headers: response.headers
        }
    }, function(error, queueItem) {
        if (error) {
            return crawler.emit("queueerror", error, queueItem);
        }

        // Do we need to save cookies? Were we sent any?
        if (crawler.acceptCookies &amp;&amp; response.headers.hasOwnProperty("set-cookie")) {
            try {
                crawler.cookies.addFromHeaders(response.headers["set-cookie"]);
            } catch (error) {
<span class="apidocCodeCommentSpan">                /**
                 * Fired when an error was encountered while trying to add a
                 * cookie to the cookie jar
                 * @event Crawler#cookieerror
                 * @param {QueueItem} queueItem The queue item representing the resource that returned the cookie
                 * @param {Error} error         The error that was encountered
                 * @param {String} cookie       The Set-Cookie header value that was returned from the request
                 */
</span>                crawler.emit("cookieerror", queueItem, error, response.headers["set-cookie"]);
            }
        }

        /**
         * Fired when the headers for a request have been received
         * @event Crawler#fetchheaders
         * @param {QueueItem} queueItem           The queue item for which the headers have been received
         * @param {http.IncomingMessage} response The [http.IncomingMessage]{@link https://nodejs.org/api/http.html#http_class_http_incomingmessage
} for the request's response
         */
        crawler.emit("fetchheaders", queueItem, response);

        // We already know that the response will be too big
        if (responseLength &gt; crawler.maxResourceSize) {

            crawler.queue.update(queueItem.id, {
                fetched: true
            }, function(error, queueItem) {
                if (error) {
                    return crawler.emit("queueerror", error, queueItem);
                }

                // Remove this request from the open request map
                crawler._openRequests.splice(
                    crawler._openRequests.indexOf(response.req), 1);

                response.destroy();
                crawler.emit("fetchdataerror", queueItem, response);
            });

        // We should just go ahead and get the data
        } else if (response.statusCode &gt;= 200 &amp;&amp; response.statusCode &lt; 300) {

            async.every(crawler._downloadConditions, function(downloadCondition, callback) {
                if (downloadCondition.length &lt; 3) {
                    try {
                        callback(null, downloadCondition(queueItem, response));
                    } catch (error) {
                        callback(error);
                    }
                } else {
                    downloadCondition(queueItem, response, callback);
                }
            }, function(error, result) {

                if (error) {
                    /**
                     * Fired when a download condition returns an error
                     * @event Crawler#downloadconditionerror
                     * @param {QueueItem} queueItem The queue item that was processed when the error was encountered
                     * @param {*}         error
                     */
                    crawler. ...</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.mimeTypeSupported" id="apidoc.element.simplecrawler.crawler.prototype.mimeTypeSupported">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>mimeTypeSupported
        <span class="apidocSignatureSpan">(mimetype)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">mimeTypeSupported = function (mimetype) {
    var crawler = this;

    return crawler.supportedMimeTypes.some(function(mimeCheck) {
        if (typeof mimeCheck === "string") {
            return mimeCheck === mimetype;
        }

        return mimeCheck.test(mimetype);
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.processURL" id="apidoc.element.simplecrawler.crawler.prototype.processURL">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>processURL
        <span class="apidocSignatureSpan">(url, referrer)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">processURL = function (url, referrer) {
    var newUrl,
        crawler = this;

    if (typeof referrer !== "object") {
        referrer = {
            url: crawler.initialURL,
            depth: QUEUE_ITEM_INITIAL_DEPTH - 1
        };
    }

    // If the URL didn't contain anything, don't fetch it.
    if (!(url &amp;&amp; url.trim().length)) {
        return false;
    }

    // Check if querystring should be ignored
    if (crawler.stripQuerystring === true) {
        url = uri(url).search("").href();
    }

    if (crawler.stripWWWDomain &amp;&amp; url.match(/https?:\/\/(www\.).*/i)) {
        url = url.replace("www.", "");
    }

    try {
        newUrl = uri(url).absoluteTo(referrer.url).normalize();

        if (crawler.urlEncoding === "iso8859") {
            newUrl = newUrl.iso8859();
        }
    } catch (e) {
        // Couldn't process the URL, since URIjs choked on it.
        return false;
    }

    // simplecrawler uses slightly different terminology to URIjs. Sorry!
    return {
        host:      newUrl.hostname(),
        path:      newUrl.resource(),
        port:      newUrl.port(),
        protocol:  newUrl.protocol() || "http",
        uriPath:   newUrl.path(),
        url:       newUrl.href(),
        depth:     referrer.depth + 1,
        referrer:  referrer.url,
        fetched:   false,
        status:    "created",
        stateData: {}
    };
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.protocolSupported" id="apidoc.element.simplecrawler.crawler.prototype.protocolSupported">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>protocolSupported
        <span class="apidocSignatureSpan">(URL)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">protocolSupported = function (URL) {
    var protocol,
        crawler = this;

    try {
        protocol = uri(URL).protocol();

        // Unspecified protocol. Assume http
        if (!protocol) {
            protocol = "http";
        }

    } catch (e) {
        // If URIjs died, we definitely /do not/ support the protocol.
        return false;
    }

    return crawler.allowedProtocols.some(function(protocolCheck) {
        return protocolCheck.test(protocol);
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.queueLinkedItems" id="apidoc.element.simplecrawler.crawler.prototype.queueLinkedItems">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>queueLinkedItems
        <span class="apidocSignatureSpan">(resourceData, queueItem)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">queueLinkedItems = function (resourceData, queueItem) {
    var crawler = this;

    var resources = crawler.discoverResources(resourceData.toString(), queueItem);
    resources = crawler.cleanExpandResources(resources, queueItem);

<span class="apidocCodeCommentSpan">    /**
     * Fired when a request times out
     * @event Crawler#fetchtimeout
     * @param {QueueItem} queueItem The queue item for which the request timed out
     * @param {Number} timeout      The delay in milliseconds after which the request timed out
     */
</span>    crawler.emit("discoverycomplete", queueItem, resources);

    resources.forEach(function(url) {
        if (crawler.maxDepth === 0 || queueItem.depth + 1 &lt;= crawler.maxDepth) {
            crawler.queueURL(url, queueItem);
        }
    });

    return crawler;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.queueURL" id="apidoc.element.simplecrawler.crawler.prototype.queueURL">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>queueURL
        <span class="apidocSignatureSpan">(url, referrer, force)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">queueURL = function (url, referrer, force) {
    var crawler = this,
        queueItem = typeof url === "object" ? url : crawler.processURL(url, referrer);

    // URL Parser decided this URL was junky. Next please!
    if (!queueItem) {
        return false;
    }

    // Check that the domain is valid before adding it to the queue
    if (!crawler.domainValid(queueItem.host)) {
<span class="apidocCodeCommentSpan">        /**
         * Fired when a resource wasn't queued because of an invalid domain name
         * @event Crawler#invaliddomain
         * @param {QueueItem} queueItem The queue item representing the disallowed URL
         */
</span>        crawler.emit("invaliddomain", queueItem);
        return false;
    }

    if (!crawler.urlIsAllowed(queueItem.url)) {
        /**
         * Fired when a resource wasn't queued because it was disallowed by the
         * site's robots.txt rules
         * @event Crawler#fetchdisallowed
         * @param {QueueItem} queueItem The queue item representing the disallowed URL
         */
        crawler.emit("fetchdisallowed", queueItem);
        return false;
    }

    async.every(crawler._fetchConditions, function(fetchCondition, callback) {
        if (fetchCondition.length &lt; 3) {
            try {
                callback(null, fetchCondition(queueItem, referrer));
            } catch (error) {
                callback(error);
            }
        } else {
            fetchCondition(queueItem, referrer, callback);
        }
    }, function(error, result) {
        if (error) {
            /**
             * Fired when a fetch condition returns an error
             * @event Crawler#fetchconditionerror
             * @param {QueueItem} queueItem The queue item that was processed when the error was encountered
             * @param {*}         error
             */
            crawler.emit("fetchconditionerror", queueItem, error);
            return false;
        }

        if (!result) {
            /**
             * Fired when a fetch condition prevented the queueing of a URL
             * @event Crawler#fetchprevented
             * @param {QueueItem} queueItem      The queue item that didn't pass the fetch conditions
             * @param {Function}  fetchCondition The first fetch condition that returned false
             */
            crawler.emit("fetchprevented", queueItem);
            return false;
        }

        crawler.queue.add(queueItem, force, function(error) {
            if (error) {
                if (error.code &amp;&amp; error.code === "DUPLICATE") {
                    /**
                     * Fired when a new queue item was rejected because another
                     * queue item with the same URL was already in the queue
                     * @event Crawler#queueduplicate
                     * @param {QueueItem} queueItem The queue item that was rejected
                     */
                    return crawler.emit("queueduplicate", queueItem);
                }

                /**
                 * Fired when an error was encountered while updating a queue item
                 * @event Crawler#queueerror
                 * @param {QueueItem} error     The error that was returned by the queue
                 * @param {QueueItem} queueItem The queue item that the crawler tried to update when it encountered the error
                 */
                return crawler.emit("queueerror", error, queueItem);
            }

            /**
             * Fired when an item was added to the crawler's queue
             * @event Crawler#queueadd
             * @param {QueueItem} queueItem The queue item that was added to the queue
             * @param {QueueItem} referrer  The queue item representing the resource where the new queue item was found
             */
            crawler.emit("queueadd", queueItem, referrer);
        });
    });

    return true;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...

To add items to the queue, use `crawler.queueURL`. This method takes 3
arguments: a URL to queue, a referrer queue item and a boolean that indicates
whether the URL should be queued regardless of whether it already exists in the
queue or not.

```js
crawler.<span class="apidocCodeKeywordSpan">queueURL</span>("/example.html", referrerQueueItem, false);
```

### Queue items

Because when working with simplecrawler, you'll constantly be handed queue items,
it helps to know what's inside them. These are the properties every queue item
is expected to have:
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.removeDownloadCondition" id="apidoc.element.simplecrawler.crawler.prototype.removeDownloadCondition">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>removeDownloadCondition
        <span class="apidocSignatureSpan">(index)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">removeDownloadCondition = function (index) {
    var crawler = this;

    if (index instanceof Function) {
        var itemIndex = crawler._downloadConditions.indexOf(index);
        return Boolean(crawler._downloadConditions.splice(itemIndex, 1));
    } else if (typeof index === "number") {
        return Boolean(crawler._downloadConditions.splice(index, 1));
    }

    throw new Error("Unable to find indexed download condition");
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
function listener(queueItem, response, callback) {
    // Do something
}

var conditionID = myCrawler.addDownloadCondition(listener);

// By id...
myCrawler.<span class="apidocCodeKeywordSpan">removeDownloadCondition</span>(conditionID);
// or by reference
myCrawler.removeDownloadCondition(listener);
```

## The queue

Like any other web crawler, simplecrawler has a queue. It can be directly
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.removeFetchCondition" id="apidoc.element.simplecrawler.crawler.prototype.removeFetchCondition">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>removeFetchCondition
        <span class="apidocSignatureSpan">(index)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">removeFetchCondition = function (index) {
    var crawler = this;

    if (index instanceof Function) {
        var itemIndex = crawler._downloadConditions.indexOf(index);
        return Boolean(crawler._fetchConditions.splice(itemIndex, 1));
    } else if (typeof index === "number") {
        return Boolean(crawler._fetchConditions.splice(index, 1));
    }

    throw new Error("Unable to find indexed fetch condition");
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
function listener(queueItem, stateData) {
    // Do something
}

var conditionID = myCrawler.addFetchCondition(listener);

// By id...
myCrawler.<span class="apidocCodeKeywordSpan">removeFetchCondition</span>(conditionID);
// or by reference
myCrawler.removeFetchCondition(listener);
```

## Download conditions

While fetch conditions let you determine which resources to put in the queue,
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.start" id="apidoc.element.simplecrawler.crawler.prototype.start">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>start
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">start = function () {
    var crawler = this;

    if (crawler.running) {
        return crawler;
    }

    crawler.running = true;

    var queueItem = crawler.processURL(crawler.initialURL);
    queueItem.referrer = undefined;
    queueItem.depth = QUEUE_ITEM_INITIAL_DEPTH;

    crawler.queue.add(queueItem, false, function(error) {
        if (error &amp;&amp; error.code !== "DUPLICATE") {
            throw error;
        }

        process.nextTick(function() {
            crawler.crawlIntervalID = setInterval(crawler.crawl.bind(crawler),
                crawler.interval);

            crawler.crawl();
        });

<span class="apidocCodeCommentSpan">        /**
         * Fired when the crawl starts. This event gives you the opportunity to
         * adjust the crawler's configuration, since the crawl won't actually start
         * until the next processor tick.
         * @event Crawler#fetchstart
         */
</span>        crawler.emit("crawlstart");
    });

    return crawler;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
```

Then, when you're satisfied and ready to go, start the crawler! It'll run
through its queue finding linked resources on the domain to download, until it
can't find any more.

```js
crawler.<span class="apidocCodeKeywordSpan">start</span>();
```

## Events

simplecrawler's API is event driven, and there are plenty of events emitted
during the different stages of the crawl. Arguments passed to events are written
in parentheses.
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.stop" id="apidoc.element.simplecrawler.crawler.prototype.stop">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>stop
        <span class="apidocSignatureSpan">(abortRequestsInFlight)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">stop = function (abortRequestsInFlight) {
    var crawler = this;
    clearInterval(crawler.crawlIntervalID);
    crawler.running = false;

    // If we've been asked to terminate the existing requests, do that now.
    if (abortRequestsInFlight) {
        crawler._openRequests.forEach(function(request) {
            request.abort();
        });
    }

    return crawler;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.urlIsAllowed" id="apidoc.element.simplecrawler.crawler.prototype.urlIsAllowed">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>urlIsAllowed
        <span class="apidocSignatureSpan">(url)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">urlIsAllowed = function (url) {
    var crawler = this;

    var formattedURL = uri(url).normalize().href(),
        allowed = false;

    // The punycode module sometimes chokes on really weird domain
    // names. Catching those errors to prevent crawler from crashing
    try {
        allowed = crawler._robotsTxts.reduce(function(result, robots) {
            var allowed = robots.isAllowed(formattedURL, crawler.userAgent);
            return result !== undefined ? result : allowed;
        }, undefined);
    } catch (error) {
        // URL will be avoided
    }

    return allowed === undefined ? true : allowed;
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.crawler.prototype.wait" id="apidoc.element.simplecrawler.crawler.prototype.wait">
        function <span class="apidocSignatureSpan">simplecrawler.crawler.prototype.</span>wait
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">wait = function () {
    var crawler = this,
        cleared = false,
        timeout =
            setTimeout(function() {
                if (cleared) {
                    return;
                }
                cleared = true;
                crawler._openListeners--;
            }, crawler.listenerTTL);

    crawler._openListeners++;

    return function() {
        if (cleared) {
            return;
        }
        cleared = true;
        crawler._openListeners--;
        clearTimeout(timeout);
    };
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
until either you execute the callback it returns, or a timeout is reached
(configured in `crawler.listenerTTL`, by default 10000 ms.)

#### Example asynchronous event listener

```js
crawler.on("fetchcomplete", function(queueItem, data, res) {
    var continue = this.<span class="apidocCodeKeywordSpan">wait</span>();
    doSomeDiscovery(data, function(foundURLs) {
        foundURLs.forEach(crawler.queueURL.bind(crawler));
        continue();
    });
});
```
...</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.queue" id="apidoc.module.simplecrawler.queue">module simplecrawler.queue</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.queue.queue" id="apidoc.element.simplecrawler.queue.queue">
        function <span class="apidocSignatureSpan">simplecrawler.</span>queue
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">queue = function () {
    Array.call(this);

<span class="apidocCodeCommentSpan">    /**
     * Speeds up {@link FetchQueue.oldestUnfetchedItem} by storing the index at
     * which the latest oldest unfetched queue item was found.
     * @name FetchQueue._oldestUnfetchedIndex
     * @private
     * @type {Number}
     */
</span>    Object.defineProperty(this, "_oldestUnfetchedIndex", {
        enumerable: false,
        writable: true,
        value: 0
    });

    /**
     * Serves as a cache for what URL's have been fetched. Keys are URL's,
     * values are booleans.
     * @name FetchQueue._scanIndex
     * @private
     * @type {Object}
     */
    Object.defineProperty(this, "_scanIndex", {
        enumerable: false,
        writable: true,
        value: {}
    });

    /**
     * Controls what properties can be operated on with the
     * {@link FetchQueue#min}, {@link FetchQueue#avg} and {@link FetchQueue#max}
     * methods.
     * @name FetchQueue._allowedStatistics
     * @type {Array}
     */
    Object.defineProperty(this, "_allowedStatistics", {
        enumerable: false,
        writable: true,
        value: [
            "actualDataSize",
            "contentLength",
            "downloadTime",
            "requestLatency",
            "requestTime"
        ]
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.super_" id="apidoc.element.simplecrawler.queue.super_">
        function <span class="apidocSignatureSpan">simplecrawler.queue.</span>super_
        <span class="apidocSignatureSpan">()</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">function Array() { [native code] }</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>


</div>

<div class="apidocSectionDiv">
<h1><a href="#apidoc.module.simplecrawler.queue.prototype" id="apidoc.module.simplecrawler.queue.prototype">module simplecrawler.queue.prototype</a></h1>


    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.add" id="apidoc.element.simplecrawler.queue.prototype.add">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>add
        <span class="apidocSignatureSpan">(queueItem, force, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">add = function (queueItem, force, callback) {
    var queue = this;

    function addToQueue() {
        queue._scanIndex[queueItem.url] = true;
        queueItem.id = queue.length;
        queueItem.status = "queued";
        queue.push(queueItem);
        callback(null, queueItem);
    }

    queue.exists(queueItem.url, function(err, exists) {
        if (err) {
            callback(err);
        } else if (!exists) {
            addToQueue();
        } else if (force) {
            if (queue.indexOf(queueItem) &gt; -1) {
                callback(new Error("Can't add a queueItem instance twice. You may create a new one from the same URL however."));
            } else {
                addToQueue();
            }
        } else {
            var error = new Error("Resource already exists in queue!");
            error.code = "DUPLICATE";
            callback(error);
        }
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
var jar = this;

if (!Array.isArray(headers)) {
    headers = [headers];
}

headers.forEach(function(header) {
    jar.<span class="apidocCodeKeywordSpan">add</span>(header);
});

if (callback instanceof Function) {
    callback(null);
}

return jar;
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.avg" id="apidoc.element.simplecrawler.queue.prototype.avg">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>avg
        <span class="apidocSignatureSpan">(statisticName, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">avg = function (statisticName, callback) {
    var sum = 0,
        count = 0,
        queue = this;

    if (queue._allowedStatistics.indexOf(statisticName) === -1) {
        return callback(new Error("Invalid statistic"));
    }

    queue.forEach(function(item) {
        if (item.fetched &amp;&amp; Number.isFinite(item.stateData[statisticName])) {
            sum += item.stateData[statisticName];
            count++;
        }
    });

    callback(null, sum / count);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
```js
crawler.queue.max("requestLatency", function(error, max) {
    console.log("The maximum request latency was %dms.", max);
});
crawler.queue.min("downloadTime", function(error, min) {
    console.log("The minimum download time was %dms.", min);
});
crawler.queue.<span class="apidocCodeKeywordSpan">avg</span>("actualDataSize", function(error, avg) {
    console.log("The average resource size received is %d bytes.", avg);
});
```

For general filtering or counting of queue items, there are two methods:
`crawler.queue.filterItems` and `crawler.queue.countItems`. Both take an object
comparator and a callback.
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.countItems" id="apidoc.element.simplecrawler.queue.prototype.countItems">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>countItems
        <span class="apidocSignatureSpan">(comparator, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">countItems = function (comparator, callback) {
    this.filterItems(comparator, function(error, items) {
        if (error) {
            callback(error);
        } else {
            callback(null, items.length);
        }
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
```

For general filtering or counting of queue items, there are two methods:
`crawler.queue.filterItems` and `crawler.queue.countItems`. Both take an object
comparator and a callback.

```js
crawler.queue.<span class="apidocCodeKeywordSpan">countItems</span>({ fetched: true }, function(error, count) {
    console.log("The number of completed items is %d", count);
});

crawler.queue.filterItems({ status: "notfound" }, function(error, items) {
    console.log("These items returned 404 or 410 HTTP statuses", items);
});
```
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.defrost" id="apidoc.element.simplecrawler.queue.prototype.defrost">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>defrost
        <span class="apidocSignatureSpan">(filename, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">defrost = function (filename, callback) {
    var queue = this,
        defrostedQueue = [];

    fs.readFile(filename, function(err, fileData) {
        if (err) {
            return callback(err);
        }

        if (!fileData.toString("utf8").length) {
            return callback(new Error("Failed to defrost queue from zero-length JSON."));
        }

        try {
            defrostedQueue = JSON.parse(fileData.toString("utf8"));
        } catch (error) {
            return callback(error);
        }

        queue._oldestUnfetchedIndex = defrostedQueue.length - 1;
        queue._scanIndex = {};

        for (var i = 0; i &lt; defrostedQueue.length; i++) {
            var queueItem = defrostedQueue[i];
            queue.push(queueItem);

            if (queueItem.status === "queued") {
                queue._oldestUnfetchedIndex = Math.min(queue._oldestUnfetchedIndex, i);
            }

            queue._scanIndex[queueItem.url] = true;
        }

        callback(null, queue);
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
otherwise you'll get an empty file.

```js
crawler.queue.freeze("mysavedqueue.json", function () {
    process.exit();
});

crawler.queue.<span class="apidocCodeKeywordSpan">defrost</span>("mysavedqueue.json");
```

## Cookies

simplecrawler has an internal cookie jar, which collects and resends cookies
automatically and by default. If you want to turn this off, set the
`crawler.acceptCookies` option to `false`. The cookie jar is accessible via
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.exists" id="apidoc.element.simplecrawler.queue.prototype.exists">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>exists
        <span class="apidocSignatureSpan">(url, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">exists = function (url, callback) {
    if (this._scanIndex[url]) {
        callback(null, 1);
    } else {
        callback(null, 0);
    }
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.filterItems" id="apidoc.element.simplecrawler.queue.prototype.filterItems">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>filterItems
        <span class="apidocSignatureSpan">(comparator, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">filterItems = function (comparator, callback) {
    var items = this.filter(function(queueItem) {
        return compare(comparator, queueItem);
    });

    callback(null, items);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
comparator and a callback.

```js
crawler.queue.countItems({ fetched: true }, function(error, count) {
    console.log("The number of completed items is %d", count);
});

crawler.queue.<span class="apidocCodeKeywordSpan">filterItems</span>({ status: "notfound" }, function(error, items) {
    console.log("These items returned 404 or 410 HTTP statuses", items);
});
```

The object comparator can also contain other objects, so you may filter queue
items based on properties in their `stateData` object as well.
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.freeze" id="apidoc.element.simplecrawler.queue.prototype.freeze">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>freeze
        <span class="apidocSignatureSpan">(filename, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">freeze = function (filename, callback) {
    var queue = this;

    // Re-queue in-progress items before freezing...
    queue.forEach(function(item) {
        if (item.fetched !== true) {
            item.status = "queued";
        }
    });

    fs.writeFile(filename, JSON.stringify(queue, null, 2), function(err) {
        callback(err);
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
commences and stops is perfectly reasonable.

Note that the methods themselves are asynchronous, so if you are going to exit
the process after you do the freezing, make sure you wait for callback -
otherwise you'll get an empty file.

```js
crawler.queue.<span class="apidocCodeKeywordSpan">freeze</span>("mysavedqueue.json", function () {
    process.exit();
});

crawler.queue.defrost("mysavedqueue.json");
```

## Cookies
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.get" id="apidoc.element.simplecrawler.queue.prototype.get">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>get
        <span class="apidocSignatureSpan">(index, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">get = function (index, callback) {
    var queue = this;

    queue.getLength(function(error, length) {
        if (error) {
            callback(error);
        } else if (index &gt;= length) {
            callback(new RangeError("Index was greater than the queue's length"));
        } else {
            callback(null, queue[index]);
        }
    });
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
Like any other web crawler, simplecrawler has a queue. It can be directly
accessed through `crawler.queue` and implements an asynchronous interface for
accessing queue items and statistics. There are several methods for interacting
with the queue, the simplest being `crawler.queue.get`, which lets you get a
queue item at a specific index in the queue.

```js
crawler.queue.<span class="apidocCodeKeywordSpan">get</span>(5, function (queueItem) {
    // Do something with the queueItem
});
```

*All queue method are in reality synchronous by default, but simplecrawler is
built to be able to use different queues that implement the same interface, and
those implementations can be asynchronous - which means they could eg. be backed
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.getLength" id="apidoc.element.simplecrawler.queue.prototype.getLength">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>getLength
        <span class="apidocSignatureSpan">(callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">getLength = function (callback) {
    callback(null, this.length);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
    var originalEmit = crawler.emit;
    crawler.emit = function(evtName, queueItem) {
        crawler.queue.countItems({ fetched: true }, function(err, completeCount) {
            if (err) {
throw err;
            }

            crawler.queue.<span class="apidocCodeKeywordSpan">getLength</span>(function(err, length) {
if (err) {
    throw err;
}

console.log("fetched %d of %d  %d open requests, %d open listeners",
    completeCount,
    length,
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.max" id="apidoc.element.simplecrawler.queue.prototype.max">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>max
        <span class="apidocSignatureSpan">(statisticName, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">max = function (statisticName, callback) {
    var maximum = 0,
        queue = this;

    if (queue._allowedStatistics.indexOf(statisticName) === -1) {
        return callback(new Error("Invalid statistic"));
    }

    queue.forEach(function(item) {
        if (item.fetched &amp;&amp; item.stateData[statisticName] &gt; maximum) {
            maximum = item.stateData[statisticName];
        }
    });

    callback(null, maximum);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
* `actualDataSize`

You can get the maximum, minimum, and average values for each with the
`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions
respectively.

```js
crawler.queue.<span class="apidocCodeKeywordSpan">max</span>("requestLatency", function(error, max) {
console.log("The maximum request latency was %dms.", max);
});
crawler.queue.min("downloadTime", function(error, min) {
console.log("The minimum download time was %dms.", min);
});
crawler.queue.avg("actualDataSize", function(error, avg) {
console.log("The average resource size received is %d bytes.", avg);
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.min" id="apidoc.element.simplecrawler.queue.prototype.min">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>min
        <span class="apidocSignatureSpan">(statisticName, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">min = function (statisticName, callback) {
    var minimum = Infinity,
        queue = this;

    if (queue._allowedStatistics.indexOf(statisticName) === -1) {
        return callback(new Error("Invalid statistic"));
    }

    queue.forEach(function(item) {
        if (item.fetched &amp;&amp; item.stateData[statisticName] &lt; minimum) {
            minimum = item.stateData[statisticName];
        }
    });

    callback(null, minimum === Infinity ? 0 : minimum);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions
respectively.

```js
crawler.queue.max("requestLatency", function(error, max) {
    console.log("The maximum request latency was %dms.", max);
});
crawler.queue.<span class="apidocCodeKeywordSpan">min</span>("downloadTime", function(error, min) {
    console.log("The minimum download time was %dms.", min);
});
crawler.queue.avg("actualDataSize", function(error, avg) {
    console.log("The average resource size received is %d bytes.", avg);
});
```
...</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.oldestUnfetchedItem" id="apidoc.element.simplecrawler.queue.prototype.oldestUnfetchedItem">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>oldestUnfetchedItem
        <span class="apidocSignatureSpan">(callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">oldestUnfetchedItem = function (callback) {
    var queue = this;

    for (var i = queue._oldestUnfetchedIndex; i &lt; queue.length; i++) {
        if (queue[i].status === "queued") {
            queue._oldestUnfetchedIndex = i;
            callback(null, queue[i]);
            return;
        }
    }

    // When no unfetched queue items remain, we previously called back with an
    // error, but since it's not really an error condition, we opted to just
    // call back with (null, null) instead
    callback(null, null);
}</pre></li>
    <li>example usage<pre class="apidocCodePre">n/a</pre></li>
    </ul>



    <h2>
        <a href="#apidoc.element.simplecrawler.queue.prototype.update" id="apidoc.element.simplecrawler.queue.prototype.update">
        function <span class="apidocSignatureSpan">simplecrawler.queue.prototype.</span>update
        <span class="apidocSignatureSpan">(id, updates, callback)</span>
        </a>
    </h2>
    <ul>
    <li>description and source-code<pre class="apidocCodePre">update = function (id, updates, callback) {
    var queue = this,
        queueItem;

    for (var i = 0; i &lt; queue.length; i++) {
        if (queue[i].id === id) {
            queueItem = queue[i];
            break;
        }
    }

    if (!queueItem) {
        callback(new Error("No queueItem found with that URL"));
    } else {
        deepAssign(queueItem, updates);
        callback(null, queueItem);
    }
}</pre></li>
    <li>example usage<pre class="apidocCodePre">...
// Trim whitespace. If no path is present - assume index.html.
var sanitisedPath = path.length ? path.replace(/\s*$/ig, "") : "index.html";
var headers = queueObject.stateData.headers, sanitisedPathParts;

if (sanitisedPath.match(/\?/)) {
    sanitisedPathParts = sanitisedPath.split(/\?/g);
    var resource = sanitisedPathParts.shift();
    var hashedQS = crypto.createHash("sha1").<span class="apidocCodeKeywordSpan">update</span>(sanitisedPathParts.join
("?")).digest("hex");
    sanitisedPath = resource + "?" + hashedQS;
}

pathStack = sanitisedPath.split(/\//g);
pathStack = pathStack.map(function(pathChunk) {
    if (pathChunk.length &gt;= 250) {
        return crypto.createHash("sha1").update(pathChunk).digest("hex");
...</pre></li>
    </ul>


</div>

<div class="apidocFooterDiv">
    [ this document was created with
    <a href="https://github.com/kaizhu256/node-utility2" target="_blank">utility2</a>
    ]
</div>
</div>
</body></html>